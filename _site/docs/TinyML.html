<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>TinyML | AESD-Course-Project</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="TinyML" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Course Project Documentation Page for UCB’s AESD Sp `21" />
<meta property="og:description" content="Course Project Documentation Page for UCB’s AESD Sp `21" />
<link rel="canonical" href="http://localhost:4000/docs/TinyML.html" />
<meta property="og:url" content="http://localhost:4000/docs/TinyML.html" />
<meta property="og:site_name" content="AESD-Course-Project" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="TinyML" />
<script type="application/ld+json">
{"headline":"TinyML","@type":"WebPage","url":"http://localhost:4000/docs/TinyML.html","description":"Course Project Documentation Page for UCB’s AESD Sp `21","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="http://localhost:4000/">AESD-Course-Project</a></h1>
        
        

        <p>Course Project Documentation Page for UCB's AESD Sp `21</p>

        

        

        
      </header>
      <section>

      <h2 id="what-is-tiny-machine-learning-tinyml">What is Tiny Machine Learning (TinyML)</h2>
<p>Tiny machine learning, or TinyML, is an emerging field that is at the intersection of machine learning and embedded systems. TinyML is all about taking just a small, little computing device (Micro-Controller) and enabling it with machine learning smarts. It is a fast-growing field of machine learning technologies and applications that include algorithms, hardware, and software that are capable of performing on-device sensor data analytics, doing so at extremely low power, typically in the order of milliwatts, so that we can enable always-on machine learning use cases on battery-powered devices.</p>

<!-- https://hbr.org/webinar/2017/04/whats-your-data-strategy -->
<!-- https://www.mckinsey.com/~/media/McKinsey/Industries/Technology%20Media%20and%20Telecommunications/High%20Tech/Our%20Insights/The%20Internet%20of%20Things%20The%20value%20of%20digitizing%20the%20physical%20world/The-Internet-of-things-Mapping-the-value-beyond-the-hype.pdf -->

<h2 id="tiny-machine-learning-with-tensorflow">Tiny Machine Learning with Tensorflow</h2>

<!-- Insert TinyML.png Image here  -->
<p><img src="images/TinyML.png" alt="Tiny Machine Learning" /></p>

<p>While  <a href="https://www.tensorflow.org/">TensorFlow</a> is written with fast custom C++ code under the hood, it has a high level Python API. A custom neural network using Tensorflow, <a href="https://www.tensorflow.org/">TensorFlow Lite</a> and <a href="https://www.tensorflow.org/lite/microcontrollers">TensorFlow Micro</a></p>

<!-- Insert Integration TinyML image here  -->
<p><img src="images/TinyML_integration.png = 100x100" alt="Integration" /></p>

<p>TinyML helps to provide a unique solution by summarizing and analyzing data at the edge on low power embedded devices, TinyML can provide smart summary statistics that take these previously lost patterns, anomalies, and advanced analytics into account.</p>

<h2 id="what-it-takes-to-enable-tinyml">What it takes to Enable TinyML</h2>

<p><a href="https://support.google.com/websearch/answer/2940021?co=GENIE.Platform%3DAndroid&amp;hl=en">Okay Google</a>, <a href="https://machinelearning.apple.com/research/hey-siri">Hey Siri</a>, <a href="https://developer.amazon.com/en-US/alexa">Alexa</a> are all successful applications of TinyML, where the devices are used as Inference engine, designed to react to perform an action for a specific set of intructions to showcase output, or act as inference engine.</p>

<!-- Insert EnableTinyML.png image here  -->
<p><img src="images/EnableTinyML.png" alt="Integration" /></p>

<h3 id="input-to-the-embedded-systems-enabled-ml">Input to the Embedded Systems enabled ML</h3>
<p>Usually the input to the above mechanism are sensors, these can be Motion Sensors, Acoustic Sensors, Temperature, Humidity, environment sensorss, touchscreen input, Cameras, Biometric sensors and force or rotation sensors.</p>

<h3 id="processing-input">Processing Input</h3>
<p>In the simple example of alexa this can be a audio input or incasse of a Robot this can be an Inertial Measurement Unit sensor. Usually the objective of these small processors on these embedded devices to act a inference engine in Milliwatts range, to either send the analysis undertaken over to the clouds or some servers usually deisgned to perform Software as a service.</p>

<h3 id="generating-output-post-processing">Generating Output Post Processing</h3>

<p>In have physical actuation by, like, activating servos, for instance. Or one might be able to trigger speakers, generate some kind of signal. In the case “Hey, Siri,” that’s effectively what we’re doing. The machine is waking up and it might say “hello” or something to that effect. But it’s not always about physical actuation of physical devices.One can also have digital actuations, that is input process
and extrapolation some kind of interesting data. And you send that digital signal out to a screen. And then that’s useful information.</p>

<h2 id="embedded-boards-under-analysis">Embedded Boards under analysis</h2>

<table>
  <thead>
    <tr>
      <th>Platforms</th>
      <th>NVIDIA Jetson Nano</th>
      <th>Arduino Nano 33 BLE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Compute</td>
      <td>472 GFLOPS</td>
      <td>1Mhz - 400 Mhz</td>
    </tr>
    <tr>
      <td>Memory</td>
      <td>2GB -4 GB</td>
      <td>2KB-512KB</td>
    </tr>
    <tr>
      <td>Power</td>
      <td>10 W</td>
      <td>150 - 23.5 mW</td>
    </tr>
  </tbody>
</table>

<!-- Insert Images of Arduino and Jetson here  -->

<h2 id="machine-learning-algorithm-challenges-embedded-perspective">Machine Learning Algorithm Challenges (Embedded Perspective)</h2>

<!-- Insert Parameter_Traning.png Image here -->
<p><img src="images/Parameter_Traning.png" alt="Parameter Traning" /></p>

<p>In order to get to obtain good accuracy and deisgn analysis, modern nueral network frameworks are growing in complexity from deeper neural networks to higher precision floating point arithmetic which requires astounding amount of compute power with GPU’s, TPU’s  and smart FPU’s. These capabilities are not almost not present in ubiquitous embedded systems.</p>

<!-- Insert GFLOPS_VS_MODELs_Param.png -->
<p><img src="images/GFLOPS_VS_MODELs_Param.png" alt="GFLOPS Vs Models" />
<!-- Source: S. Bianco, R. Cadene, L. Celona and P. Napoletano, "Benchmark Analysis of Representative Deep Neural Network Architectures," in IEEE Access, vol. 6, pp. 64270-64277, 2018, doi: 10.1109/ACCESS.2018.2877890. --></p>

<p>From the above figure lets focus onto <a href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet</a>, which happened in 2012 which was used to predict a thousand classes from <a href="http://www.image-net.org/">ImageNet</a> data set. It had an  accuracy of 57.1%. And its model size was 61 megabytes in size. To obtain better accuracy  <a href="https://arxiv.org/abs/1409.1556">VGGNet</a> came along in 2014 which boosted the accuracy to 71.5%. But with that came a boost from 60 megabytes model size to 528 megabytes.Additionally in 2015, Microsoft released <a href="https://arxiv.org/abs/1512.03385">ResNet</a>, residual nets with accuracy of 75.8%
while shrinking the model size as it was getting better. In order to make them both accurate and also be more cognizant of the size.</p>

<p><a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md">MobileNet</a> for example is used in smartphones due to its optimization for memeory at the capacity of smaller disk usage which is hardly 17 megabytes, with 75% accuracy. The key nugget is that our little embedded microcontrollers onlyhave a few kilobytes of memory. With a few kilobytes of memory, it’s an order of magnitude difference between where usually state of the art sits and how we need to cram things in.</p>

<!-- Insert Transform.png -->
<p><img src="images/Transform.png" alt="GFLOPS Vs Models" /></p>

<h2 id="machine-learning-lifecycle">Machine Learning Lifecycle</h2>
<p>A classic Machine learning course is about Statistics and algorithm development for analysis and design of Neural networks and performing tools developement to optimzie sections of the neural network but this only form the ML code box. To deploy machine learning in production environemnt has multiple inputs which can lead to multiple outputs.</p>

<h3 id="data-engineering">Data Engineering</h3>
<p>Data engineering is all about defining what are the data requirements. It consists of</p>
<ul>
  <li>Defining Data requirements : Spectators, observes, Data in Male and Female voice etc.</li>
  <li>Collecting Data</li>
  <li>Data Labelling and Pre-processing: Data Collected can be noisy and may hamper performance for the ML Code box, this forms a vital step with direct implication on inference.</li>
  <li>Data Preparation and Data Augmentation: Classical Machine Learning techniques involve splitting the data into testing, validation and training.</li>
  <li>Repeat the above process</li>
</ul>

<h3 id="model-engineering">Model Engineering</h3>
<p>Model engineering is about design and performance analysis.</p>
<ul>
  <li>Traning ML Models: This forms the actual traning phase of the model and Weight parameterization and design.</li>
  <li>Improving Traning Speeds: Deep neural networks can take days/months depending on compute capacity and hardware. Traning speed optimizations can be huge bottlenecks for TinyML.</li>
  <li>Target Metrics: Metrics based on how accurate and at what resolution the expected out can satisfy product needs (Application Use case).</li>
  <li>Evaluation against metrics and standards: The model designed has to robust to uncertainty, which can be difficult in machine learning and more so on low power embedded devices.</li>
  <li>Model Optimization: All the steps lead to model optimation and is a cyclic process which may or may not change with information from the clients using the TinyML product.</li>
</ul>

<h3 id="model-deployment">Model Deployment</h3>
<p>The Model trained at the Data Centres or with sources of higher compute have to deployed on small embedded devices</p>
<ul>
  <li>Model Conversion: Conversion of the models can be undetaken both from model design and memory mapping.
    <ul>
      <li><a href="https://towardsdatascience.com pruning-deep-neural-network-56cae1ec5505">Neural Network Pruning</a></li>
      <li><a href="https://www.tensorflow.org/lite/performance/post_training_quantization">Precision Quantization</a></li>
      <li><a href="https://keras.io/examples/vision/knowledge_distillation/">Knowledge Distillation</a></li>
    </ul>
  </li>
  <li>Performace and energy aware optimization: Tiny Machine Learning application have to be fast to respond. Latency Vs Accuracy can be pivotal in the success of the application and more so if power and energy usage have to be limited to the micro-controller under use.</li>
  <li>Inference serving API’s: Tensorflow/PyTorch/Keras are standard libraries which are used to create portable Tiny Machine Learning models.</li>
</ul>

<h3 id="product-analytics">Product Analytics</h3>
<p>This section of ML Architecture ususally consists of creating dashboards (<a href="https://www.tensorflow.org/tensorboard">Tensorboard</a>), on-field evaluations and inference and optimazations.</p>

<!-- Insert MLPipeline.png -->
<p><img src="images/MLPipeline.png" alt="Machine Learning Pipeline" /></p>

<h2 id="usecase-of-tensorflow">Usecase of Tensorflow</h2>

<!-- Inensorflow_pipeline.png -->
<p><img src="images/Tensorflow_pipeline.png" alt="Tesnorflow Pipeline in use for this Project" />
The figure above shows the workflow methodology we will be following for building the tinyML applications in this course. These are broken down into three main stages:</p>

<ul>
  <li>Step 1: Collect &amp; Preprocess Data</li>
  <li>Step 2: Design and Train a Model</li>
  <li>Step 3: Evaluate, Optimize, Convert and Deploy Model</li>
</ul>

<h2 id="steps-taken-for-conversion-in-this-project">Steps Taken for conversion in this Project</h2>

<p>Assumption: A general assumption for the reader for this section, is no Tensorflow based API’s are discussed here, it is assumed that a model development, neural network design and data engineering is undertakena is application specific. We have used <a href="https://cocodataset.org/#home">COCO Dataset</a>, and has been trained under <a href="https://github.com/arpit6232/visualwakeup_aesd">MobileNet for Binary Classification</a> if the camera frame can detect to see it a person is in the frame of reference.</p>

<h3 id="example-conversion-process">Example Conversion Process</h3>

<h4 id="how-to-use-tflite-models">How to use TFLite Models</h4>
<ul>
  <li>Convert Model API
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
tflite_model = converter.convert()
</code></pre></div>    </div>
  </li>
  <li>Write to disk
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pathlib
tflite_model_file = pathlib.Path('model.tflite')
tflite_model_file.write_bytes(tflite_model)
</code></pre></div>    </div>
  </li>
  <li>Use a pre-saved tflite file, you then instantiate a tf.lite.Interpreter, and use the ‘model_content’ property to specify an existing model
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interpreter = tf.lite.Interpreter(model_path=tflite_model_file)
</code></pre></div>    </div>
  </li>
  <li>Once you’ve loaded the model you can then start performing inference with it.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
to_predict = # Input data in the same shape as what the model expects
interpreter.set_tensor(input_details[0]['index'], to_predict)
tflite_results = interpreter.get_tensor(output_details[0]['index'])
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="quantization-aware-training">Quantization Aware Training</h4>
<p>There are two primary forms of quantization – <a href="https://www.tensorflow.org/lite/performance/post_training_quantization">post training quantization</a> where, as part of the conversion process, your model’s internal weights and ops get converted to int8 and uint8 and <a href="https://www.tensorflow.org/model_optimization/guide/quantization/training">quantization aware training</a></p>

<p>To further optimize your model, we explored quantization aware training. These concepts are explained in detail later in this README</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Non-quantized Top-1 Accuracy</th>
      <th>8-bit Quantized Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>MobilenetV1 224</td>
      <td>71.03%</td>
      <td>71.06%</td>
    </tr>
    <tr>
      <td>Resnet v1 50</td>
      <td>76.3%</td>
      <td>76.1%</td>
    </tr>
    <tr>
      <td>MobilenetV2 224</td>
      <td>70.77%</td>
      <td>70.01%</td>
    </tr>
  </tbody>
</table>

<h5 id="post-training-quantization">Post Training Quantization</h5>

<!-- Insert Post_training_quantization.png -->
<p><img src="images/Post_training_quantization.png=250x250" alt="PostTraining Quantization" /></p>

<p>Reducing Float Numbers to int8 can lead upto 4X reduction in memory. In a machine learning inference engine like Arduno BLE 33 has 256 KB of RAM is not entirely accessible for user space programs to use. Thus having minimum size of the machine learning model is crutial. This mathematical papers dives deeper into how Gaussian analysis proves that this conversion holds (<a href="https://arxiv.org/abs/1510.00149">Resarch_Paper</a>), which also talks that it speeds up inferece making.</p>

<p>In summary, 
Doing Calculations in 8-bit integers offers some compelling advantages:</p>
<ul>
  <li>Faster arithmatic: Fewer gates to implement.</li>
  <li>Lower memory demands.</li>
  <li>Reduces resource requirements: Many low-end micro-controllers and DSP’s lack floating-point hardware, so avoiding floats increases portability.</li>
</ul>

<h5 id="quantization-aware-training-1">Quantization Aware Training</h5>
<p>Post tranining quantization leaks error into inference due to loss of precision.<a href="https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html">Quantization-aware training</a> effectively emulates the inference-time quantization behavior by creating a model the downstream tools will use to produce the actual quantized models. The quantized models, naturally, will be using lower precision.</p>

<p>So in order to introduce the error into the training graph to become resilient to them later, what we want to do is we want to modify the original neural network graph that we typically have in floating-point values so that when we’re doing the computations, we are implicitly quantizing the values. And when we do the calculations, the natural output that goes out when we’re doing the weights times the inputs plus the biases, that computation effectively should have the error
that we are going to have in the post-training quantization. And by injecting that error directly into the training pipeline, what happens is a network is only seeing that error be propagated through. And so when it does its back propagation and it does its gradient descent and the loss functions are working on it,it’s naturally going to try and become resilient to that issue.</p>

<p>And so effectively, by exposing the training pipeline to the error, we’re going to be able to naturally get the neural network to learn to improve its accuracy by itself.</p>

<h2 id="why-are-8-bits-are-enough-for-ml">Why are 8-Bits are Enough for ML?</h2>
<p>During training neural networks the biggest challenge is getting them to work at all. That means tackling accuracy and speed. Using floating-point arithmetic was the easiest way to preserve accuracy, and GPUs were well-equipped to accelerate those calculations, so it’s natural that not much attention was paid to other numerical formats.</p>

<p>Floating Point Arithmatic is generally not present on ubiquitus micro-controllers. That is where quantization comes in. It’s an umbrella term that covers a lot of different techniques to store numbers and perform calculations on them in more compact formats than 32-bit floating-point.</p>

<h3 id="why-does-quantization-work">Why does Quantization Work</h3>
<p>Neural networks are trained through stochastic gradient descent. These small increments typically need floating-point precision to work, otherwise, one can get yourself into a pickle with such things as “vanishing gradients.”. Taking a pre-trained model and running inference is very different. One can think about recognizing an object in a photo you’ve just taken, the network has to ignore all the CCD noise, lighting changes, and other non-essential differences between it and the training examples it’s seen before, and focus on the important similarities instead.</p>

<p>This ability means that they seem to treat low-precision calculations as just another source of noise, and still produce accurate results even with numerical formats that hold less information. One can run many neural networks with eight-bit parameters and intermediate buffers (instead of full precision 32-bit floating-point values), and suffer no noticeable loss in the final accuracy. There might suffer a little bit of loss in accuracy but often the gains you get in terms of performance latency and memory bandwidth are justifiable.</p>

<h3 id="why-quantize">Why Quantize</h3>
<ul>
  <li>
    <p>Quantization is to shrink file sizes by storing the min and max for each layer and then compressing each float value to an eight-bit integer representing the closest real number in a linear set of 256 within the range. This means you can get the benefit of a file on disk that’s shrunk by 75%, and then convert back to float after loading so that your existing floating-point code can work without any changes.</p>
  </li>
  <li>
    <p>Reduction in computational resources to do the inference calculations, by running them entirely with eight-bit inputs and outputs is much more profitable financially and for engineering.</p>
  </li>
</ul>

<h2 id="tensorflow-vs-tensorflow-lite">Tensorflow Vs Tensorflow Lite</h2>

<p>To do justice to this revolutionary topic, this article <a href="Understand TensorFlow by mimicking its API from scratch">Medium</a>. Exaplins the difference between tensorFlow and TensorFlow Lite.</p>

<h1 id="visual-wake-works">Visual Wake Works</h1>
<p>In this project, the objective is to use a camera attached to the Arduino BLE 33 (Micro-controller) to detect a person in the key-frame.</p>

<h3 id="visual-wake-works-dataset">Visual Wake Works Dataset</h3>
<p>Visual Wake Words is used to embedded and computer science realm to speak of activating a device to perform some action upon visualizing (through a camera) on an embedded platform.</p>

<h4 id="data-collection-and-pre-processing">Data Collection and Pre-Processing</h4>
<p>Microsoft MSCOCO Dataset is used here for training MobileNet and transfer learning is used over TensorFlow 2.0, using <a href="https://arxiv.org/abs/1906.05721">Visual Wake Words Dataset</a>.</p>

<h4 id="neural-network-design-for-visual-wake-words">Neural Network Design for Visual Wake Words</h4>
<p>We want a network that is small and parameters and to make sure it runs efficiently. <a href="https://arxiv.org/abs/1704.04861">MobileNet</a> is an industry standards generally used on smart phones and is tested to work with over 70% accuracy with low memory requirements and is used in this project.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Size</th>
      <th>Top-1 Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>MobilenetV1 224</td>
      <td>16 MB</td>
      <td>71.3%</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Further Optimization Undertaken
    <ul>
      <li><a href="https://paperswithcode.com/method/depthwise-convolution">Depth Multiplier Convolution</a></li>
      <li>Alpha Optimization - which controlls number of paramters needed to define the model vs accuracy.</li>
      <li><a href="https://paperswithcode.com/task/architecture-search">Neural Architecture Search</a></li>
    </ul>
  </li>
</ul>

<h4 id="metrics-for-visual-wake-words-evaluation">Metrics for Visual Wake Words (Evaluation)</h4>
<p>Qualitative and quantitative metrics for VWW applications which is fairly commonly used in Computer Vision applications, which are Latency vs Accuracy tradeoffs, Fairness and Variety of Dataset collected.</p>



      </section>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
